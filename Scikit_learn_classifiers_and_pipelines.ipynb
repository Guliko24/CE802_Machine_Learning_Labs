{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guliko24/CE802_Machine_Learning_Labs/blob/main/Scikit_learn_classifiers_and_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVtKWdBBc5t-"
      },
      "source": [
        "# Prediction with sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy2QgtGXc5uA"
      },
      "source": [
        "By: Jacobo Fernandez-Vargas and Luca Citi\n",
        "\n",
        "Based on the [scikit-learn documentation](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-TFldfdc5uB"
      },
      "source": [
        "## Loading the 'digits' dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sMXYZ9yc5uB"
      },
      "source": [
        "Scikit-learn comes with a few standard datasets, for instance the *Iris* and *digits* datasets for classification and the *Boston house prices* dataset for regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Re8G8Goc5uB"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "digits = datasets.load_digits()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYga7VEIc5uC"
      },
      "source": [
        "A dataset is a dictionary-like object that holds all the data and some metadata about the data. The data is stored in the `.data` member, which is usually a `n_samples` by `n_features` array. In the case of supervised problem, one or more response variables are stored in the `.target` member."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EkbZDDOc5uC"
      },
      "source": [
        "For instance, in the case of the digits dataset, `digits.data` gives access to the features that can be used to classify the digits samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhvWEiLBc5uC"
      },
      "outputs": [],
      "source": [
        "print(digits.data.shape)\n",
        "print(digits.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iCo9Ny1c5uD"
      },
      "source": [
        "and `digits.target` gives the ground truth for the digit dataset, that is the number corresponding to each digit image that we are trying to learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNhlnk4hc5uD"
      },
      "outputs": [],
      "source": [
        "print(digits.target.shape)\n",
        "print(digits.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UDerZ3ic5uD"
      },
      "source": [
        "## Learning and predicting\n",
        "In the case of the digits dataset, the task is to predict, given an image, which digit it represents. We are given samples of each of the 10 possible classes (the digits 0 through 9). These can be used to fit an estimator to predict the class an unseen example belongs to.\n",
        "\n",
        "In scikit-learn, an estimator for classification is a Python object that implements the methods `fit(X, y)` and `predict(T)`. If you remember in the last lab we saw that the preprocessing classes had the methods `fit(X, y)` and `transform(T)`. We will see the differences later.\n",
        "\n",
        "An example of an estimator is the class `sklearn.svm.SVC` that implements support vector classification. The constructor of an estimator takes as arguments the parameters of the model, but for the time being, we will consider the estimator as a black box:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYnYoaltc5uE"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC(gamma=0.0001, C=100.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OhrfyyJc5uE"
      },
      "source": [
        "We call our estimator instance `clf`, as it is a classifier. **It now must be fitted to the model, that is, it must learn from the data**. This is done by passing our training set to the `fit` method. As a training set, let us use all the images of our dataset apart from the last one. We select this training set with the `[:-1]` Python syntax, which produces a new array that contains **all but the last entry** of `digits.data`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzVDvLOvc5uE"
      },
      "outputs": [],
      "source": [
        "clf.fit(digits.data[:-1], digits.target[:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JVxs8ylc5uF"
      },
      "source": [
        "Now you can predict new values. In particular, we can ask to the classifier to recognise the digit of our last image in the digits dataset, which we have not used to train the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1vOtAnXc5uF"
      },
      "outputs": [],
      "source": [
        "clf.predict(digits.data[-1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXhsKSw-c5uF"
      },
      "source": [
        "The corresponding image is the following.\n",
        "As you can see, it is a challenging task: the images are of poor resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFCKpUu6c5uF"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.matshow(digits.images[-1], cmap='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmx6puDNc5uF"
      },
      "source": [
        "## Grid-search\n",
        "\n",
        "Almost all classifiers have parameters that need to be set. The meaning and roles of these pararmeters differ largely between classifiers and their appropiate selection can have huge impact on the results (but some times they may have almost no effect). This parameter fitting should happen inside a cross-validation process. Scikit-learn provides `GridSeachCV` that, given data, fit of an estimator on each combination of parameters on parameter grid and then chooses the parameters to maximize the cross validation score. To use this, we need to specify the classifier that we want to use and the values of the parameters that we want to test (through a dictionary). Interestingly, the  `GridSeachCV` constructor creates an object that can be used as a predcictor, using its `fit`, `predict`, `score`, ... methods; in other words it is a meta-predictor (i.e. a preditor built from other predictors, an SVM in thisd case) that can be used like any other predictors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWA4uf6Dc5uG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "clf = svm.SVC(gamma=0.01, C=10.)\n",
        "Cs = np.logspace(-1, 3, 5)  # five logarithmically spaced values of C\n",
        "Gs = np.logspace(-7, -0, 5)  # five logarithmically spaced values of gamma\n",
        "clf = GridSearchCV(estimator=clf, param_grid=dict(C=Cs, gamma=Gs), n_jobs=-1)\n",
        "\n",
        "clf.fit(digits.data, digits.target)\n",
        "clf.score(digits.data, digits.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc9zGioac5uG"
      },
      "source": [
        "**IMPORTANT NOTE**\n",
        "\n",
        "The way we trained and tested (fit, score) is WRONG! You should not calculate the score with the same data used for training, as we saw in the previous lectures and lab. We did it this way for demonstration purposes of how the GridSeachCV class works. The fact that we see a score of 1.0 (100%) should make us suspicious: it's often a sign that we did something wrong (either that or the problem is trivial)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fAKfLJtc5uG"
      },
      "source": [
        "We can see how the different values of C and gamma affect the performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBb8S5GGc5uG"
      },
      "outputs": [],
      "source": [
        "# Grid results for last fold\n",
        "scores = clf.cv_results_['mean_test_score'].reshape(len(Cs), len(Gs))\n",
        "extent = np.log10([Gs[0], Gs[-1], Cs[0], Cs[-1]])\n",
        "im = plt.imshow(scores, extent=extent, origin='lower')\n",
        "plt.colorbar(im)\n",
        "plt.contour(np.log10(Gs), np.log10(Cs), scores)\n",
        "plt.xlabel('log10(Gamma)')\n",
        "plt.ylabel('log10(C)')\n",
        "print('Best parameters: ', clf.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKDHe-BJc5uG"
      },
      "source": [
        "## Automated cross validation\n",
        "\n",
        "In the previous lab we saw the importance of using fold validation and we saw a way of implementing it. Fortunately, sklearn provides us with tools that take care of most of the heavy lifting: `cross_val_score` and `cross_val_predict`. The former returns only the score (the error function value, for example MSE), while the latter returns all the predicted values. In general, the score is enough to validate a model and compare different options. However, having all the predicted values may be useful in some cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj0WzgZKc5uH"
      },
      "source": [
        "The following code may take some time as it is performing nested cross-validation. This is because the `cross_val_score` function performs cross-validation on a meta-predictor that internally performs another cross-validation loop. The result is a nested cross-validation where `cross_val_score` performs the outer loop for performance estimation and the `GridSeachCV` object takes care of the inner one for parameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNLIqKQIc5uH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "score = cross_val_score(clf, digits.data, digits.target, cv=5)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAhsPLbyc5uH"
      },
      "source": [
        "As you can see the score from the fold validation varies fold to fold and it is not 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_724ya2c5uH"
      },
      "source": [
        "## Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bZUq0ric5uH"
      },
      "source": [
        "One of the most flexible tools provided by Scikit-learn are the pipelines. These allow us to create sequential steps of preprocessing (those that have `fit` and `transform` methods) followed by a classifier (that has `fit` and `predict` methods). From the point of view of Scikit-learn, the pipeline itself is a classifier, meaning that we can use directly the grid search and the cross validation classes. (To speed up the processing, we are fixing the SVM's C parameter and tuning gamma only)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC53-T2cc5uH"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "clf = svm.SVC(gamma=0.01, C=10.)\n",
        "pipe = Pipeline(steps=[('scaler', sc), ('classifier', clf)])\n",
        "# Parameters of pipelines can be set using ‘__’ between the name of the pipeline block and its parameter names:\n",
        "param_grid = {\n",
        "    'scaler__with_std': [True, False],\n",
        "    'classifier__gamma': np.logspace(-4, -3, 3),\n",
        "}\n",
        "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
        "score = cross_val_score(search, digits.data, digits.target)\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_-nBZpCc5uI"
      },
      "source": [
        "In this particular case, the inclusion of a scaler did not have any impact on the results (because gray levels are somewhat already normalised and possibly because we are not tuning C), but it may be extremely helpful in other cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3r51PhLc5uI"
      },
      "source": [
        "# Testing different classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTtJwX5Uc5uI"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0-jYlEac5uI"
      },
      "source": [
        "Try other classifiers to solve the problem and compare their results. Suggestions for classifiers: Logistic Regression, Decision Trees. You will need to look up the documentation to find which are the parameters to be fit as well as the different classes available. Once you do this, you will see how easy is to change the model and test different approaches with this set up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoZuqoMGc5uI"
      },
      "source": [
        "# Regression and unsupervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8eoCPxGc5uI"
      },
      "source": [
        "In this lab we have seen how to use sklearn to solve a classification problem, which is why we used support vector machines. However, everything that we have seen here can be applied to both regression and (to some extent) unsupervised learning. Depending on the problem we will need to vary the model used for the prediction but most of the concepts seen on this lab also apply to those cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKGabpjHc5uJ"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2tdtVcSc5uJ"
      },
      "source": [
        "Load the boston dataset form sklearn `datasets.load_boston` and use what you have learned to solve a regression problem."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}